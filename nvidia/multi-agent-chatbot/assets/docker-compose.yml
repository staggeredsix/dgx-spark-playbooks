#
# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
services:
  backend:
    container_name: backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./media:/app/media
    depends_on:
      flux-service:
        condition: service_started
      video-service:
        condition: service_started
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=chatbot
      - POSTGRES_USER=chatbot_user
      - POSTGRES_PASSWORD=chatbot_password
      - NEO4J_URI=neo4j://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=chatbot_neo4j
      - NEO4J_DATABASE=neo4j
      - LLM_API_BASE_URL=http://ollama:11434/v1
      - EMBEDDING_MODEL=qwen3-embedding:8b
      - CODE_MODEL=qwen3-coder:30b
      - VISION_MODEL=ministral-3:14b
      - MODELS=gpt-oss:120b,ministral-3:14b,qwen3-coder:30b
      - FLUX_SERVICE_URL=http://flux-service:8080
      - VIDEO_SERVICE_URL=http://video-service:8081

  ollama:
    image: ollama/ollama:0.13.4
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    gpus: all
    environment:
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_AUTOSTART_MODELS=gpt-oss:120b,qwen3-coder:30b,ministral-3:14b,qwen3-embedding:8b
    volumes:
      - ollama-data:/root/.ollama

  frontend:
    container_name: frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    restart: unless-stopped
    environment:
      - NEXT_PUBLIC_BACKEND_PROTOCOL=${BACKEND_PROTOCOL:-http}
      - NEXT_PUBLIC_BACKEND_HOST=${BACKEND_HOST:-backend}
      - NEXT_PUBLIC_BACKEND_PORT=${BACKEND_PORT:-8000}
      - BACKEND_PROTOCOL=${BACKEND_PROTOCOL:-http}
      - BACKEND_HOST=${BACKEND_HOST:-backend}
      - BACKEND_PORT=${BACKEND_PORT:-8000}


  flux-service:
    container_name: flux-service
    build:
      context: ./flux-service
      dockerfile: Dockerfile
    environment:
      - FLUX_MODEL_ID=${FLUX_MODEL_ID:-black-forest-labs/FLUX.1-schnell}
      - FLUX_MODEL_DIR=${FLUX_MODEL_DIR:-/models/flux-schnell}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - FLUX_OUTPUT_DIR=${FLUX_OUTPUT_DIR:-/outputs}
    ports:
      - "8080:8080"
    volumes:
      - ./flux-schnell:/models/flux-schnell
      - hf-cache:/models/hf_cache
      - flux-outputs:/outputs
    restart: unless-stopped
    gpus: all

  video-service:
    container_name: video-service
    build:
      context: ./video-service
      dockerfile: Dockerfile
    environment:
      - WAN_MODEL_VARIANT=${WAN_MODEL_VARIANT:-t2v-A14B}
      - WAN_CKPT_DIR=${WAN_CKPT_DIR:-/models/wan2.2/ckpt}
      - WAN_CODE_DIR=${WAN_CODE_DIR:-/opt/wan2.2}
      - WAN_OUT_DIR=${WAN_OUT_DIR:-/tmp/wan_out}
      - WAN_PRECACHE=${WAN_PRECACHE:-true}
      - WAN_TIMEOUT_S=${WAN_TIMEOUT_S:-1800}
      - ENABLE_VOICE_TO_VIDEO=${ENABLE_VOICE_TO_VIDEO:-0}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
    ports:
      - "8081:8081"
    volumes:
      - wan-models:/models
      - wan-cache:/models/hf_cache
    restart: unless-stopped
    gpus: all


  postgres:
    container_name: postgres
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=chatbot
      - POSTGRES_USER=chatbot_user
      - POSTGRES_PASSWORD=chatbot_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chatbot_user -d chatbot"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  neo4j:
    container_name: neo4j
    image: neo4j:5.25.1-community
    restart: unless-stopped
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/chatbot_neo4j
      - NEO4J_server_jvm_additional=-XX:+ExitOnOutOfMemoryError
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -a neo4j://localhost:7687 -u neo4j -p chatbot_neo4j 'RETURN 1'"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  postgres_data:
  ollama-data:
  hf-cache:
  wan-cache:
  neo4j_data:
  neo4j_logs:
  wan-models:
  flux-outputs:

networks:
  default:
    name: chatbot-net
